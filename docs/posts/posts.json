[
  {
    "path": "posts/2023-06-01-controlling-via-residualising/",
    "title": "Controlling via residualising",
    "description": {},
    "author": [
      {
        "name": "Rogier Kievit",
        "url": {}
      }
    ],
    "date": "2023-06-01",
    "categories": [],
    "contents": "\r\nthis may or may not be useful - often reviewers might ask you to control for a certain variable. You can do this within the model, but another way is to residualize a variable for a covariate beforehand. In this toy example, you have height and weight, and perhaps you want to use one variable without the potential confounding of the other. With this v basic script you can see how to do it, and also how to use these new variables in plotting (e.g. by showing who is taller/heavier than expected given a model)\r\nit’s pretty basic but rarely illustrated (as far as I’ve seen) so may be worthwhile as an example, as I find myself showing this to many and varied people (in fact I just found an old script where I did almost the same :) )\r\n\r\n[1] 0.3827534\r\n\r\nCall:\r\nlm(formula = weight ~ height)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-33.422  -6.446   0.012   7.267  24.449 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -2.2631    16.7642  -0.135    0.893    \r\nheight        0.4136     0.1008   4.101 8.49e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 10.04 on 98 degrees of freedom\r\nMultiple R-squared:  0.1465,    Adjusted R-squared:  0.1378 \r\nF-statistic: 16.82 on 1 and 98 DF,  p-value: 8.489e-05\r\n[1] 0.9238506\r\n[1] 1.274337e-17\r\n\r\nCall:\r\nlm(formula = height ~ weight)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-27.5327  -5.9086   0.6549   6.0076  26.7652 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 142.42605    5.80651  24.529  < 2e-16 ***\r\nweight        0.35419    0.08636   4.101 8.49e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 9.293 on 98 degrees of freedom\r\nMultiple R-squared:  0.1465,    Adjusted R-squared:  0.1378 \r\nF-statistic: 16.82 on 1 and 98 DF,  p-value: 8.489e-05\r\n[1] 0.9238506\r\n\r\n\r\nAuthor: Rogier Kievit\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-01-controlling-via-residualising/2023-06-01-controlling-via-residualising_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-06-01T09:47:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-23-rainclouds/",
    "title": "From Raincloud Plots 1.0 to Raincloud Plots 2.0 to …",
    "description": {},
    "author": [
      {
        "name": "Jordy van Langen",
        "url": {}
      },
      {
        "name": "Nick Judd",
        "url": {}
      }
    ],
    "date": "2023-03-23",
    "categories": [],
    "contents": "\r\nWhile the days are still short and dark, this post tries to spark some light by taking you along in our exciting open-science project: Raincloud plots!\r\nThe beginning - version 1.0\r\nSomewhere during 2018, a fellow cognitive neuroscientist from Aarhus University, Micah Allen, was experimenting with a new type of data visualization on Twitter (the better Twitter times, Ed.). He was doing so in the R programming language and - crucially - data visualization is one of the many reasons to use R.\r\n\r\n\r\n\r\nFigure 1: For the historical record, Jon Roiser coined the term ‘raincloud plots’.\r\n\r\n\r\n\r\nAfter some ‘90 degrees’ rotating and ‘name-coining’, Raincloud Plots were born after which Micah wrote a blogpost on his website (https://neuroconscience.wordpress.com/2018/03/15/introducing-raincloud-plots/).\r\nShortly after, more people started to get involved such as Davide Poggialli who replicated it in Python, Tom Rhys Marshall recreated these in Matlab, and Kirstie Whitaker who wrapped the various codebases into a Jupyter notebook.\r\nFrom Tweets to Papers\r\nThe first instance of the Raincloud plots project took solely place in Twitter DMs which eventually ended up in writing a peer-reviewed paper, or more specifically a ‘software tool article’ that was published in Wellcome Open Research. After a little more than a year, we submitted a revised version of the paper and incorporated the most requested feature, a fully operational ‘raincloudplots’ R-package (https://github.com/jorvlan/raincloudplots).\r\nUpgrade to version 2.0\r\nIn 2021, the Dutch research council (NWO) announced the launch of their inaugural Open Science Fund. NWO set up the Open Science Fund as a way to recognise and reward open science practices by supporting projects by researchers who are (or want to be) frontrunners in this movement. Part of the assessment, therefore, included applicants’ open science track record, which counted for 10% of the assessment. Team members Rogier and Jordy applied and raincloudplots were selected. One cool feature of this funding round is that all proposals and review comments were, in principle, made publicly available, showing how Open Science can also be part of the funding cycle! https://www.nwo.nl/en/news/open-science-fund-project-proposals-published-today. There was considerable interest in this first round of the Open Science Fund. A total of 167 admissible applications were assessed, 26 of which were granted.\r\nSome months later, the project took off and we were given the opportunity to present our project plans during the NWO Open Science webinar series. While talking about the scientific impact of our project, Rogier confessed to the audience some (ambiguous) self-reflection:\r\n[…] “It is either good or depressing that this will likely be one of the most impactful things I did in my career”[…]\r\nAfter a good laugh and a fruitful discussion about the history and future of our project and about open science in general, the session ended and we started preparing the next phase of our project.\r\n\r\n\r\n\r\nFigure 2: NWO Open Science in Practice Webinar Series, Thursday, 14 April 2022\r\n\r\n\r\n\r\nSlides and recording of the session can be found here:\r\n* https://www.nwo.nl/sites/nwo/files/media-files/Open%20Science%20Presentation%20Rogier%20Kievit_raincloudplots.pdf\r\n* https://www.youtube.com/watch?v=Kvcyh_9KSbw&t=1910s)\r\nA new team member\r\nIn September 2022, the project team was strengthened with Nicholas Judd, a former visiting PhD student in Rogier’s lab. Now being fully operational, we started organizing online, globally accessible workshops and initiated developing our second raincloudplots R-package, coined ‘ggrain’ (https://github.com/njudd/ggrain). The latter is a ggplot2 extension R-package that allows users to create Raincloud plots - following the ‘Grammar of Graphics’. The package can do a myriad of things, but most importantly it:\r\nIs highly customizable\r\nConnects longitudinal observations\r\nHandles Likert data\r\nAllows mapping of a covariate.\r\nFor a complete overview of ggrain such as a 2-by-2 raincloud plot or multiple repeated measures, please see our Vignette.\r\n\r\n\r\n\r\nFigure 3: An example plot from ‘’ggrain’’.\r\n\r\n\r\n\r\nThus far, we have organized 3 workshops which received positive reviews and a lot of useful feedback, such as suggesting to include automatic significance testing using ‘geom_signif’. Below is a list of social media outputs from the workshop:\r\nhttps://twitter.com/SportSciSum/status/1591135705702473728\r\nhttps://twitter.com/rogierK/status/1573269145617063936\r\nhttps://twitter.com/ioannik23/status/1591101555998326784\r\n\r\n\r\n\r\nFigure 4: The fourth and final workshop is scheduled for February 17th, 14:00 CET.\r\n\r\n\r\n\r\nNWO Open Science day - Utrecht office\r\nAs part of the awarded grant, we were invited to the NWO Open Science Meetup, to share knowledge, gain inspiration and celebrate the award. After a word of welcome by NWO’s Executive Board, two members of the Open Science Fund review committee shared their experiences during the review process. Other parts that were discussed:\r\nThe high quality of the submitted projects and ’a worrying research waste: data that is neither used for publications nor shared.\r\nThe importance of placing the users – the community – at the center and addressing their real needs. In this respect, the Open Science Fund is a wonderful example of an innovative and pragmatic programme, which is sorely needed in the transition to open science.\r\nDiversity and Inclusion, Open Data Sharing and Re-use, Open Source Software, Rewards and Incentives for Open Science, Digital Infrastructure for Open Science, Open Reproducible Research, Community Building and Engagement.\r\n\r\n\r\n\r\nFigure 5: The NWO Open Science Meetup, June 24th 2022.\r\n\r\n\r\n\r\nJASP\r\nTogether with the highly popular JASP Statistics (https://jasp-statistics.org) we are further developing Raincloud Plots in their interface. JASP is the open-source alternative to the proprietary software SPSS. JASP offers both classical (i.e, frequentist) and Bayesian analysis methods. JASP has integrated Raincloud Plots last year, hotlink: https://jasp-stats.org/2021/10/05/raincloud-plots-innovative-data-visualizations-in-jasp/.\r\nCurrently, we are extending the visualization capabilities of JASP, by adding a covariate option, likert plot and multiple time point visualizations.\r\n\r\n\r\n\r\nFigure 6: You can read the full blogpost about Raincloud Plots in JASP here: https://jasp-stats.org/2021/10/05/raincloud-plots-innovative-data-visualizations-in-jasp/\r\n\r\n\r\n\r\nFinal remarks\r\nIf you have been using our framework over the past 4 years, we sincerely would like to thank you! If this is the first time you are reading about Raincloud Plots, we hope that it will be of use for you some day! We are working to constantly improve it and your feedback is greatly appreciated.\r\nOn behalf of the entire Raincloud Plots team, our best wishes for 2023.\r\nNicholas Judd, Jordy van Langen, Rogier Kievit\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-23-rainclouds/fig1.png",
    "last_modified": "2023-03-30T15:53:28+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-16-Donders-Open-Science-Day/",
    "title": "Lowering Barriers to Entry: Reflections on the Donders Open Science Day 2022",
    "description": {},
    "author": [
      {
        "name": "Sam Parsons",
        "url": {}
      }
    ],
    "date": "2022-11-16",
    "categories": [],
    "contents": "\r\nOpen Science is the hip thing right now, confirmed our first speaker of the day, Dirk van Gorp. The Donders Session: Open Science Day certainly reinvigorated my enthusiasm for open science – and not just because it’s where all the cool kids hang out.\r\nWe (Fleur Zeldenrust, Sam Parsons, Rogier Kievit, and Laura de Nooij) wanted to showcase a wide range of useful tools and researchers’ experiences in Open Science. Seven speakers joined us in person, and remotely, across three broad sessions.\r\nIn the spirit of Open Science, recordings1 of the presentations and the slides are available here. The talks speak for themselves, but there were some key take-home messages and threads that ran neatly through the day.\r\n\r\n\r\n\r\nSession 1: “What is open science?”\r\nThere isn’t a clear answer, with “almost as many definitions as universities” (Dirk van Gorp). It was enlightening to hear about many initiatives happening at the Radboud – some behind-the-scenes, and others much more visible. The current policy initiatives look to help researchers make their work more Findable and Accessible. Dirk asked the audience who was familiar with the central Open Access policy, and only four hands were raised. This became a running theme of the event, there are many more impressive Open Science-related initiatives out there to learn about and benefit from. Making these initiatives and resources more visible is an important goal.\r\nSession 2: Our speakers introduced a bunch of useful tools\r\nGonny Kremers covered how to publish your work openly and the routes to open access, including Green (self-archiving), Gold (journal open access), and hybrid. Gonny shared important information for all Netherlands based researchers about Plan S and the Taverne Amendment. Understanding each is integral for all researchers funded by organisations that have signed Plan S, including NWO, ZonMw, and European Commission.\r\nPadraig Gleeson shared Open Source Brain, a platform for sharing and developing computational models of neural systems in an open and collaborative way. Open Neuroscience data, open models, and simulation tools are powerful, but to maximise their use, we need the computing infrastructure – Open Source Brain provides this. As Padraig said, and was echoed by almost every other speaker, “we want to lower the barrier for participation in science”.\r\nCaspar van Lissa introduced a Workflow for Open Reproducible Code in Science (WORCS), a tool to “lower the barrier of entry” into reproducible science. WORCS introduces an open and reproducible research workflow from the beginning of a project. It is built into an R package with github integration – which not only helps reproducibility but adds version control too. This has the benefit of making all of the tasks we usually remember at the last minute (share code, prepare data to share) easy and part of the natural workflow of a project.\r\n\r\n\r\n\r\nSession 3: Experiences of being an open scientist\r\nEmma Henderson shared her experience of doing only Registered Reports. For a Registered Report, authors submit a Stage 1 manuscript (introduction, methods, data analysis plan) to a journal for review. The manuscript is reviewed and accepted in-principle, before data are collected. Then, after the data are collected and analyses ran, the manuscript is accepted regardless of the results. This exciting format removes many forms of bias from the publication process, chiefly whether manuscripts are accepted based on significant results. The most common worry for students and their supervisors, interested in registered reports, is time. Emma’s response was clearly “I can make this work” – she completed three for her PhD Thesis. More than that, the review times Emma shared made many of us jealous compared to “traditional” peer review timelines.\r\nStephanie Forkel shared a smorgasbord of principles and practices of Open Science she uses with her collaborators: Use open tools (e.g. human connectome project, FSL, python), Share your data (e.g. neurovault, github, thingiverse, and Stephanie’s personal website), and make manuscripts open access (e.g. medRxiv, bioRxiv, Research Square, and more). In keeping with the spirit of the day, Stephanie discussed taking everyone else along and equitable access with the Neuroscience Alliance (NEURAL). To further discuss, engage, and communicate check out @CNSeminars; there are journal clubs, journal special issues, interviews, neuroimaging tutorials, movies, debates, and more. There are so many ways to get involved and benefit from Open Science that it can be overwhelming. But, don’t feel like you have to do everything – just find something that works for you.\r\nCoosje Veldkamp is a project manager of the YOUth Cohort Study. The study itself is super impressive, following nearly 4000 Dutch children throughout development from pregnancy into early adulthood. Added to this, YOUth is a trailblazer in Open Science: the data are FAIR (Findable, Accessible, Interoperable, Reusable) to allow other researchers to use them, they are harmonising these data with other large scale projects (including the Consortium on Individual Development). Coosje shared the vital behind-the-scenes innovation that was required to build and maintain this huge open longitudinal cohort. Finally, Coosje shared the sobering note that funding for YOUth will soon run out, which prompted some audience reflection on the importance of longer-term investment in important open science infrastructure projects.\r\nDiscussion: A day like this would not be complete without audience discussion and participation, and for our virtual attendees’ who doesn’t enjoy a break-out room discussion? Drawing on the Open Science as a buffet metaphor (Christina Bergman; read more here https://www.bps.org.uk/psychologist/bropenscience-broken-science) we discussed our own experiences, the kinds of open practices we want to try next, and the training we would need.\r\nThe practices that attendees were keen to adopt included sharing their data and code. We discussed sensitive data as a barrier to sharing, and that a viable work-around is to share simulated or synthetic data that share key statistical characteristics with the real data but are entirely anonymous. There were some fears around sharing code; what if my code is too messy? What if someone finds an error? In the end, we agreed that sharing messy code is still better - whether for others or for ourselves in the future - and at least if an error is found it can be corrected. Rounding off the discussion, we agreed that resources or training in code and data sharing would help alleviate some of these fears to help folk adopt more open practices. Perhaps for the next Donders Open Science day!\r\nAuthor: Sam Parsons\r\n\r\nThe recordings of the presentations are not available at the moment, but will be added soon.↩︎\r\n",
    "preview": "posts/2022-11-16-Donders-Open-Science-Day/open-science-Dirk_Sam.jpg",
    "last_modified": "2023-03-23T15:35:56+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-16-Null-Results-Are-Significant-Too/",
    "title": "Tell your students: Null results are significant too",
    "description": {},
    "author": [
      {
        "name": "Sam Parsons",
        "url": {}
      }
    ],
    "date": "2022-02-16",
    "categories": [],
    "contents": "\r\nWe all know that null (non-significant)1 results are informative, useful, and not at all a bad thing, right? Right…\r\nIn an entirely rigorous scientific approach to surveying peoples’ perspective of null results, I turned to statistics memes2. Just like academia, these memes neatly reflect the 5% alpha threshold: p < .05 and it’s publication party time, p > .05 and you cry a little.\r\n\r\n\r\n\r\nStudents often come face-to-face with their own personal null results towards the end of their research project. Usually the project is worth a good portion of their final grade and they feel the pressure to write a good report. When “good” includes some version of novelty or innovativeness, it’s understandable that they feel the pressure to “find something interesting” (read as: statistically significant) to discuss.\r\nSo, when the inevitable happens (p > .053), a path to the dark side emerges: Some students feel fear; what if they made a mistake or somehow ruined their study? Others feel anger, all they see are significant p values and supported hypotheses, why are they denied the right? Still more feel their burning hatred of statistics emerge again, born from a litany of frustrations throughout their training (not to mention math anxiety - see this twitter thread). Finally, many students feel suffering: at the loss of their favorite hypothesis, certainly that their training has given them little experience of describing and interpreting null results, and in feeling their “non-significant results give them nothing to write about”4.\r\nHonestly, I can’t blame them for feeling this way. The prevalence of Questionable Research Practices5 (e.g. Gopalakrishna et al. 2020) and the volume of student projects that “metamorphosize” from dull nulls into pretty supported results (e.g. O’Boyle et al. 2017) showcase that many established researchers embody the same reactions. Instead of focusing on this bleak outlook, we will keep our minds on supporting research students.\r\n\r\n\r\n\r\nA highly accurate, and well-sampled, representation of researchers’ responses to null results.\r\nMost supervisors will recognise any of the following from students’ null result papers:\r\nThe limitations clearly relate to not finding a significant effect. Sometimes the writing is explicit enough to state that fixing these limitations would help find significant effects.\r\nThey deploy the oft-used “approaching statistical significance”6 to allow them to discuss results through the more familiar lens of significant result. Again, to “have something to write about”.\r\nThe conclusions read as if nothing has been learned about the relationship between the variables, that the study itself was uninformative. Including the possibility that there is no true effect.\r\nConversely we found no effect, therefore no effect exists (and perhaps all the previous research was actually a massive false positive).\r\nI’ve often wondered how we can help our students and mentees. We can - and should - tell our trainees that a result does not have to be significant to be useful, or important, or “right”, and even publishable. But, however strongly we reinforce this, students are faced with overwhelming evidence to the contrary. Their own readings of the literature and every other subtle (or not subtle) message about new and exciting (significant) results we send them usually obliterates the message by the time they’re analyzing their hard collected data.\r\nHelping students\r\nInstead of sinking into an academic existential dread, here are two ways I have tried to help students overcome the dark side of null results (Warning: Your mileage may vary).\r\nFor several students a shallow dive into Bayes Factors7 and equivalence testing helped drastically. These projects included relatively simple models (e.g. in the realms of t-tests, correlations, and multiple regression) so the learning curve was not too steep. Thank you JASP for being an intuitive tool for students with minimal analyses and/or programming skills to run these analyses. My students were able to write that “the evidence favored the null model” instead of “we did not find a significant effect”, which seemed to empower them to “have something to talk about”. More than this, they had much more confidence in their interpretations of the study, even if that interpretation is that no hypothesis was better supported by the data.\r\nIn other student projects, we took to preregistration. Preregistration does not solve all problems (nor should it be expected to, or discussed as if it does), and much of this particular benefit could be achieved without any formal preregistration process. For us, and in more than one project, preregistration was a useful tool to explore several patterns of potential results and how we might interpret each one. It forced us to better consider the theory driving the study and how this interacted with our hypotheses and planned analyses (also see this nice blogpost for a similar non-preregistration argument). This did not entirely remove the sting when p > .058. But, those students were more capable of discussing the results in reference to the theory they were testing and the report did not read as if the study was worthless or uninformative.\r\nBroad improvements in the statistical training students undergo would also help. Whole communities have grown to support improving research training, for example a Framework for Open and Reproducible Research Training. But, waiting for the glacial pace of reforming curricula to catch up with our current needs gets old fast. Note, students likely don’t need more stats or more complex stats. Instead, they need more time dedicated to understanding how statistics work and how they can be used to make inferences.\r\nIn the meantime, I have a rapid-fire round of discussion points that have helped my students get to grips with interpreting nulls and hopefully feeling less hopelessness when they see p > .05[^And that clap of thunder makes three]. In no particular order, we could dedicate more time to; sampling variability and the dance of confidence intervals, meta analyses, what actually is a p value, effect sizes, open science, statistical power, common statistical misconceptions (Greenland et al., 2016), and that we should expect null effects in a line of studies even when the effect does exist (e.g. Lakens & Etz, 2017). Sharing papers reporting null results (maybe you have published your null results too?), gives students something tangible to grapple with other than being barraged by statistical significance. Avoiding valenced and judgemental language about results - “failed replications”, “failed experiments”, results are “negative” or “positive” - may help students feel more comfortable whatever the results. Finally, we can avoid preemptively making students feel they have missed a valuable opportunity by avoiding telling students that we might be able to publish the study “if we find something interesting”.\r\nConcluding remarks\r\nMost students have been told that their null results matter too, that null results tell us something. The often repeated phrase goes something like “even null results tell us something”. Often it feels like we are merely paying lip service to an idealized world of research and academic publishing. But, at the risk of sounding preachy, I believe that with consideration and more training we might create a world in which null results are not demonized and avoided, and instead added to the academic record to facilitate the scientific process. More immediately, maybe we can reduce some of our students’ p value anxiety.\r\nAuthor: Sam Parsons\r\nResources and links\r\nLakens, D., & Etz, A. J. (2017). Too true to be bad: When sets of studies with significant and nonsignificant findings are probably true. Social Psychological and Personality Science, 8(8), 875-881.\r\nGopalakrishna, G., Riet, G., Vink, G. Stoop, I., Wicherts, J., & Bouter, L. (2020). Prevalence of questionable research practices, research misconduct and their potential explanatory factors: a survey among academic researchers in The Netherlands. https://www.nsri2020.nl/\r\nGreenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., & Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. European journal of epidemiology, 31(4), 337–350. https://doi.org/10.1007/s10654-016-0149-3\r\nO’Boyle Jr, E. H., Banks, G. C., & Gonzalez-Mulé, E. (2017). The chrysalis effect: How ugly initial results metamorphosize into beautiful articles. Journal of Management, 43(2), 376-399.\r\nReferences\r\nJASP Materials\r\nJASP Materials\r\nFramework for Open and Reproducible Research Training (FORRT)\r\nCyrussamii\r\nCoursera - Statistical Inferences\r\n\r\nSorry, Bayesians, this might not be for you↩︎\r\nCode for reproducibility check (https://www.google.co.uk/search?q=p+value+memes). You’re welcome, Open Science.↩︎\r\nCue thunder clap and ominous music↩︎\r\nAlmost a literal quote from more than one student↩︎\r\nOr Questionable Reporting Practices, depending on your personal preference↩︎\r\nFor other amusing examples from the published literature, see https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/↩︎\r\nSee, Bayesians, I hadn’t forgotten about you↩︎\r\nCue the second thunder clap↩︎\r\n",
    "preview": "posts/2022-02-16-Null-Results-Are-Significant-Too/null_results_Sam.jpg",
    "last_modified": "2023-03-23T15:41:32+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-17-What-Is-Parsimony-Worth/",
    "title": "What is parsimony worth?",
    "description": {},
    "author": [
      {
        "name": "Michael Aristodemou",
        "url": {}
      }
    ],
    "date": "2022-01-17",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn the early 14th century, an English philosopher known as William of Ockham spoke thus: “pluralitas non est ponenda sine necessitate.” Centuries later the credo entities should not be multiplied without necessity has grown to be a universal principle, known as the principle of parsimony, which systematically biases decision making across the vast and varied scientific landscape. My introduction to this principle came during a statistics class when a senior student advised me that if I mention parsimony the department statisticians will do a little dance and award me an entire point on my assignment! Considering my main objective in life was to inflate my GPA, parsimony held a clear value. I’ve since graduated and parsimony has lost its point awarding abilities. Disappointingly, it also seems to have little effect on my supervisor’s dance tendencies. So, if parsimony cannot promote my personal goals, what is parsimony worth?\r\nParsimony in knowledge discovery\r\nThe definition of parsimony is by itself non-contentious to anyone who isn’t an avid supporter of redundancy. It also isn’t useful given that its value is context dependent. My interest lies in the domain of knowledge discovery and therefore this is the context in which parsimony will be evaluated. Let’s break down the definition and embed it into context: entities should not be multiplied without necessity. Our ultimate goal is to justify the should not part, that is to identify the utility of simplicity. To do this we first need to define the necessity. For without a target, we cannot judge when the “multiplication of entities” has to cease. Say we want to explain why people with sleep problems also experience fatigue and irritability. After we search for possible explanations, we want to select one—the best one ideally. So we need to compare our explanations, which means we need to transform them into something we can compare in terms of both accuracy and simplicity. One way to do this, is to express our theories as models and use some quantitative metric that reflects the extent to which they fit with our observations (e.g. the bayesian information criterion). We can call this model fit. Drawing from our example we can manufacture two quick-and-dirty theories: a) reciprocal causal associations between all three symptoms explain their co-occurrence; or, more “simply” b) a fourth variable (e.g., depression) causes all of them, which is why they co-occur. These explanations result in two models with different numbers of parameters. We can use the number of parameters needed to formalize an explanation to approximate its simplicity. Thus, we redefine our entities as the number of parameters needed to specify a given explanation. In the case that our models have the same fit, parsimony would dictate that we choose the simpler of the two—the more parsimonious model. Indeed, many metrics of model fit have an inbuilt penalty for the number of parameters a model has, making model fit the product of a trade-off between complexity and fit to the data1. The principle of parsimony can now be rephrased as: the number of parameters should not be multiplied further than is necessary to improve model fit. This brings us to the crux of the matter—why is this a good idea?\r\n\r\n\r\n\r\nFigure 1. Schematic of two example models one complex (Model A) and one simple (Model B)\r\nParsimony as bias\r\nIdeally, we want a model that fits both to our current observations but also fits well to independent samples. Otherwise, we run the risk of a model that fits tightly to random fluctuations in our data outperforming a model that better approximates the process of interest. This is commonly known as overfitting and is problematic since most real-world data reflects both the process of interest and sample-specific noise. To mitigate overfitting one can look at the generalization error, which indicates how well a model performs in a previously unseen sample. The rationale being that if a model performs well on a given dataset because it fits well to the sample-specific noise, then it should do poorly when fitted to another sample. While a model that fits well to the process of interest should fit well to all samples that appropriately measure that process. A common justification for choosing the most parsimonious model is that it will always minimize generalization error. Which brings us to a popular definition of parsimony: the simpler of two models with equal fit on a given dataset, will fit better on unseen datasets. This statement when taken literally is false. The falsity of this statement follows from David Wolpert’s “no free lunch” theorem2. What the “no free lunch” theorem shows is that for any pair of models there are as many domains where the simpler model is preferable as there are domains where the more complex model is preferable. This essentially invalidates the universality of simplicity as a guiding tool for model selection—in fact for any two models A and B there are as many domains where model A has lower generalization error as there are domains where this is true for model B. The more interesting question, however, isn’t whether parsimony guarantees lower generalization error. What is practically interesting is whether the use of parsimony will lead to lower generalization error in most (or all) applied situations. Domingos follows this precise question in an extensive review of empirical circumstances that contradict this weaker justification for parsimony3. Specifically, he shows that within the domain of machine learning simpler models often lead to higher generalization error. Therefore, both empirically and mathematically the claim that parsimony leads to lower generalization error is found wanting.\r\nSimplicity and comprehensibility\r\nSo what if parsimony doesn’t buy us more accurate predictions? Arguably a worthwhile cost if our main aim is to understand the world and simpler models are easier to comprehend. But what makes a model more comprehensible? In 2006, van der Maas and colleagues, debuted their theoretical alternative to the dominant g-factor model that presented a single entity as the causal driver of intelligence—much like our fictitious model B (Figure 1). The mutualism model, as van der Maas termed it, is much more complex in terms of parameters than the g-factor model and fits just as well to cross-sectional observations. If we use comprehensibility as a tie-breaker we should be safe betting on the more parsimonious model. Not so fast. Simplicity in terms of the effective number of parameters needed to specify the g-factor model does not reflect its comprehensibility. As van der Maas argues, the mutualism model certainly can multiply the number of parameters, but the g-factor model conjures up “a rather mysterious” hidden variable4. The same can be said for our toy models. Model B buys its simplicity by manufacturing an entirely new, arguably opaque entity, while model A presents a more complex mechanism that nevertheless is easier to comprehend. In other words, we can better intuit how sleeplessness might make one tired and therefore irritable, while it is harder to parse what this depression variable actually is and how it brings symptoms to be. It seems that parsimony also fails to guarantee superior comprehensibility in some domains.\r\nParsimony in model search\r\nIt seems not even parsimony suffices as a fast-and-hard rule that we can generously apply to any domain of science. But there is one domain where parsimony may still be unequivocally welcome. This is the domain of model search. Blumer showed mathematically that the more models we test on our data, the greater the chance that the best fitting model will fit poorly on other samples5. This is essentially multiple testing in model search and reflects the fact that testing more models increases the probability that we find a good fitting model purely by chance.\r\nDomingos presents an accessible overview of the mathematical argumentation in his review. It goes something like this. Let’s say that the generalization error of a hypothesis is greater than ε. From this it follows that the probability that our hypothesis is correct on x number of independent samples is smaller than (1 — ε)x[^ refers to the power of, e.g. 2^2 = 4. Our cutting-edge editor does not have superscript functionality.]. If we consider n number of hypotheses then the probability that at least one of them is correct in all x independent samples is n(1 — ε)^x. If we substitute this with real numbers, we can see that the probability of at least one of our hypotheses being correct increases with the number of hypotheses considered. That is 2(1 — 4)^2 is 18, which is smaller than 4(1 — 4)^2 which is 36. In a nutshell6, “if we select a sufficiently small set of models prior to looking at the data, and by good fortune one of those models closely agrees with the data, we can be confident that it will also do well on future data.7” Hence, multiple testing provides a safe-haven for the historic concept of parsimony, but veers from the traditional interpretation of parsimony as it pertains to the assumptions within an explanation.\r\nGoodbye parsimony\r\nThroughout my reading for this blog, I had to bury my naive belief in the powers of parsimony. That’s strike two. Now I will be forced to constrain my models in another way. Through the grind of gaining domain knowledge and thoughtfully integrating it into the a priori specification of my models. This currently seems like the only effective recourse to reduce the number of models tested and improve comprehensibility. Thanks for the points parsimony, you’ve gotten me this far…\r\nAuthor: Michael Aristodemou\r\n\r\nThis can lead to situations where parsimony leads us to choose the simpler model with worse fit to the data. Meaning parsimony is not just a tie-breaker.↩︎\r\nWolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390.↩︎\r\nWolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390.↩︎\r\nVan Der Maas, H. L., Dolan, C. V., Grasman, R. P., Wicherts, J. M., Huizenga, H. M., & Raijmakers, M. E. (2006). A dynamical model of general intelligence: the positive manifold of intelligence by mutualism. Psychological review, 113(4), 842.↩︎\r\nBlumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam’s razor. Information Processing Letters, 24, 377(380).↩︎\r\nWolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390.↩︎\r\nWolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390.↩︎\r\n",
    "preview": "posts/2022-01-17-What-Is-Parsimony-Worth/Overfitting_Michael.jpg",
    "last_modified": "2023-03-23T15:43:30+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-29-Working-from-home/",
    "title": "Working from home or living at work?",
    "description": {},
    "author": [
      {
        "name": "Rogier Kievit",
        "url": {}
      }
    ],
    "date": "2021-11-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nFriday March 6 2020 was the last day I went to my former workplace in Cambridge, UK. As we have a child who is especially vulnerable to viral illness, we decided, well before it was ‘en vogue’, to stay at home. We received rather ominous letters about us not being allowed to leave the house at all – shortly followed by more reassuring letters that ‘opening a window was allowed’.\r\nWhat many hoped would be a brief period of atypical working arrangement soon stretched to something closer to two years, and is sadly far from over. Never before have our daily work habits changed so fundamentally in such a short period of time, and the same is true for scientists. Conferences with thousands of participants became completely virtual. Lectures were commonly characterized by vast, silent plains of black squares. One is perhaps best off assuming the silence reflects a breathlessly listening audience. In the entire first year in my new post at the Donders institute I was only able to work in person a handful of times – the rest meant (re)starting a new lab in a new country from the kitchen table (not to mention buying a new house through videochat from a different country – but that’s another story). Although, unfortunately, the pandemic is far from over, we can certainly take stock of the many and rapid changes, and reflect on some of the lessons learned.\r\nThe Bad1\r\nThe challenges were, and are, many. For instance, working from home with (young) children is often almost impossible - the combination of acting as a teacher at home and at the same time juggling work responsibilities is, to put it mildly, not ideal. This is especially true for the most exciting parts of our work that demand deep thinking and reflection – Something not particularly compatible with simultaneously assisting in the construction of ambitious cardboard castles, tending to snail collections and home ‘baking’. Of course, the challenges for scientists living alone, especially those away from their home or country of origin, are distinct, but no less challenging.\r\nScientifically, one of the key drawbacks is the lack of casual, unplanned conversations at the coffee machine. These informal contacts prove not only essential as a ‘social glue’, but also as the origin of many new ideas, contacts and collaborations. As good as Zoom and team meetings have become in terms of the core business of sharing talks and slides, they generally fail at the hard-to-explain intellectual chemistry that arises from serendipitous in-person encounters. This is especially problematic for early career researchers, who build up their social and academic networks through these chance encounters. We try our best to replicate such encounters online, but the best we can do is to approximate them – or to hope to develop new strategies to foster them.\r\nAlthough the reduction of commuting time entails the clear benefit of ‘more hours in the day’, it comes with other, more psychological challenges. When I drop off my daughter in the morning and am working from home, I can be behind my desk a minute later. This sometimes whiplash inducing switch between parenting and work mode is something that takes time to master. A related challenge exists at the other end of the day – When does working from home turn to living at work? It can be overly tempting to continue working at that same kitchen table during dinner preparation, and/or well into late hours (recent research has shown workdays up to 2 hours longer for those working from home ). For long term maintenance of high level ‘deep work’ it is crucial we find rituals that allow a clear(er) division of work and free time, so that we work from home rather, than live at work, in ways sustainable in the long term. We’ve found some value in the advice and strategies in Cal Newport’s ‘Deep Work’, but many other approaches exist.\r\nThe Ugly\r\nUndoubtedly one of the ‘ugliest’ features of this pandemic is that it amplifies pre-existing inequities. Several analyses have demonstrated that the adverse consequences for scientific productivity have been especially pronounced for, among others, working parents, especially mothers, of young children2 and/or people of colour right at a career point where the proverbial pipeline is at its leakiest. As a field, we have to try our very best to consider the differential impact of these challenges at any opportunity we have, to try to adjust for the pronounced pandemic disparities.\r\nThe Good\r\nThere are, of course, also positive things to come out of our changed work habits. The ability to work remotely and flexibly, requested by people with disabilities of various kinds for decades, turned out to be quite possible once it was needed by a sufficiently large enough group of people. The decrease in commute time means that an efficient day working at home can simultaneously be more productive for work and family/leisure time – a potential win-win. And as challenging as working from home with homeschooling children has been and continues to be, it is also undoubtedly wonderful to be around them (even) more. Personally, I’ve embraced the two most cliched lockdown activities: Learning how to bake sourdough and running further than ever before, both of which are here to stay.\r\nAnother benefit is that it has become more commonplace to invite speakers from all over the world to small and large gatherings, bringing greater diversity in backgrounds and topics. Since last year we regularly invite the authors of papers in our journal clubs to join part or all of the conversation, hugely enriching the depth of our reading and understanding.\r\nHybrid and online working have become almost (but not quite) seamless, and meetings that should be zoom calls often are. I think back with some amazement and no real nostalgia at certain meetings where 20 very busy people would commute to a single location, for only 2-3 central people to speak the entire time. Similarly, the realisation that the cost in terms of CO2 and time away from loved ones for conferences at the very least means we should substantially decrease the frequency of academic activities that involve long distance travel3. In theory, this will ultimately allow a more diversified set of conferences where you can attend the majority hybrid or online, yet benefit from the very real added value of in person conferences for a subset. An added benefit of the virtual conference format is that the chat Q&A can be a great leveler - I have noticed that conference questions in chat format often come from a more diverse, more early career scientists, and are often all the more insightful, knowledgeable and creative for it.\r\nHow we’re trying to make it work\r\nAt the Donders institute, we are actively trying to learn from the recent past to improve the present as well as the more distant future. Many research groups and scientists from all backgrounds and seniorities have tried out a vast array of creative solutions – Zoom tea breaks, shut up and write sessions, gathertowns, lab walks and elaborate games, and share what works and what doesn’t. One thing my lab has initiated since the more recent tightening of restrictions has been the early morning scrum/rollcall/huddle4: Very brief (5 minute) meetings where everyone outlines their plans and we get to see everyone’s face on a more regular basis. Not only is it nice to see everyone in 2D, if not in reality, but voicing your plans for the day out loud has a surprisingly large beneficial effect on one’s productivity. Similarly, it took a while for me to be convinced of the benefits of Slack, but now that it has become fully integrated it is the virtual heart of the lab – Quick chats and meetings, a steady stream of interesting papers and resources, and attempts to decipher Dutch customs, including stroopwafel etiquette, in our #undutchables channel has become an indispensable part of the lab. Of course, all of the above is from my narrow and rather specific set of circumstances – I’d love to hear from you about challenges I have overlooked, as well as strategies that have worked well (or failed spectacularly).\r\nThe motto of my former Cambridge College (Fitzwilliam College) is ‘The best of the old and the new’. As we navigate our way through ebbing and flowing Covid waves towards a future version of academic work, I can only hope we take this motto to heart. Revive what we miss whenever possible, and embrace the new opportunities that arise.\r\nAuthor: Rogier Kievit\r\n\r\nThis blogpost is about the consequences of the pandemic for (academic) work, not the pandemic itself, but of course the single largest consequence of the pandemic is direct: (severe) illness and death for many, heartbreak for family, and exhaustion and burnout for care workers.↩︎\r\nhttps://www.nap.edu/catalog/26061/the-impact-of-covid-19-on-the-careers-of-women-in-academic-sciences-engineering-and-medicine & https://www.nature.com/articles/d41586-021-03045-w↩︎\r\ne.g. see https://academicflyingblog.wordpress.com/↩︎\r\nPlease select your least favourite term.↩︎\r\n",
    "preview": "posts/2021-11-29-Working-from-home/lockdown_Rogier.jpg",
    "last_modified": "2023-03-23T15:47:05+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-14-I-Am-Quant/",
    "title": "I am Quant (and so can you!): Seriously",
    "description": {},
    "author": [
      {
        "name": "Ethan McCormick",
        "url": {}
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nI’m sure that I am not the only one who has sat in many a conference hall (imagine back through the mists of times to those days), listening to talks and wondering “How am I ever going to be that smart/knowledgeable about [insert just about anything here]?”. As a first-year graduate student, it is perhaps unsurprising that I would feel that way. I had many years ahead of me to gain the knowledge and skills that those speakers had. All those years later, and now I’m officially “Dr. McCormick”, even though only one person ever calls me that (more on him later). However, that feeling doesn’t just go away. I regularly see a talk or read a new paper/preprint and think “Well damn, when am I going to have time to learn this?”. This has especially been true as I have attempted to pivot my research program from one aimed at addressing substantive questions regarding the brain-based changes that driving learning to one focused on studying and developing the quantitative methods we use to understand change over time (i.e., longitudinal models). I am hardly the only one who has felt the lure of advanced methods from more substantive research fields. I came of age in research during a time with a much greater focus on rigorous methods, open science, and reproducibility/replication. Many researchers in my cohort are motivated like never before to incorporate more rigorous methods from statistics and data science into their own work. However, many also find these methods intimidating. After all, how do I know that I’m using them correctly? I have been incredibly fortunate along the way to have had amazing mentors and a bit of luck in learning new methods. Here I will lay out some tips and tricks I have picked up along the way. So, if you are hoping to transition in a major way to quantitative methods, or just hoping to get better estimates for your research questions, hopefully these can help you on your way.\r\nTake a Course\r\nI know that in the age of the internet, we are all supposed to be DIYing it towards ultimate knowledge and expertise in whatever subject we take up. I have been able to use online resources to refine skills, or dive into advanced subjects, but I am not made of stern enough stuff to learn structural equation models by my little lonesome at night at my computer reading Bollen, 1989 (although I do recommend that book in the highest terms). Some sort of structured course is incredibly valuable for getting the basics under your belt, which you can then build on independently later. Of course, needing to take a formal course might be a real impediment (especially if they cost money) since not everyone is a graduate student in a quant-heavy institution. Here, some sort of online course or video series may be a great alternative, but the key is to structure it like a course, where you have periodic checks of your understanding and a committed time. Unfortunately, my intention to learn computational modeling in the evenings wasn’t successful until I registered for a course and was expected to show up every week. One free resource people might consider is the podcast Quantitude, where two quantitative faculty talk about a range of topics. It’s not systematic like a course would be, but it’s an amazing resource for those hoping to learn more about specific topics in a fun way.\r\nFind a Quantitative Person to Pester\r\nThis is only said partly in jest. When I took quant courses at UNC Chapel Hill, I frequently wanted to chase down offhanded comments by the professors. After a couple weeks of gathering my courage, I set up a meeting with them and basically came with a list of questions. This felt incredibly presumptuous and like I was wasting their time at first, but ultimately, I got more out of those conversations than I will ever be able to count. Not only did I learn more than I would have otherwise about the methods I use every day in my own research then, but those conversations also became the ideas behind manuscripts I am writing now. Like anyone, quant people love to talk about their research with an interested student, and they probably get to do it less often than others. I can’t promise that every quantitative methodologist is as lovely as the ones I have known, and time constraints are real, but you’d be surprised how many of them do want to be pestered by interested students looking to dive into advanced methods past the basic coursework.\r\nAsk Questions…so many questions\r\nThis is something I have to remind myself of even now. There is a lot of pressure to appear like you understand something (we’ve all been and seen the nodding heads in class), but there are so many things to know and the best way to accumulate them is to constantly be asking questions when you don’t understand. This also applies to professional development. I came to quant relatively late in my graduate training, but once I developed some relationships with quantitative faculty in my department, I started asking questions like “Do you think this would be a good idea for a paper?” and “Would you be willing to write a training grant with me?” I have found that the people in my career have been incredibly generous with their time and energy.\r\nThere is No Secret Sauce\r\nIf anything can be learned from my career trajectory, it’s that there is no hidden secret to getting into quantitative methods. You don’t need to be a math savant (I’m certainly not) and the new generation of methodologists is shaking off the remnants of the “old boys” club culture that was prevalent in prior decades, although much work remains to be done in that direction. Basically, I had the great fortune to study at UNC Chapel Hill with faculty like Ken Bollen, and Dan Bauer, and Patrick Curran. I was excited by what I was learning and so started to haunt the offices of Dan and Patrick. This has not only led to great mentor relationships with great researchers, but also a considerable shift in my program of research. Now instead of being the student, I help run courses and workshops in quantitative methods. If I can do it, so can you. Welcome to the quant side, we have cookies!\r\nAuthor: Ethan McCormick\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-14-I-Am-Quant/Quant_Ethan.jpg",
    "last_modified": "2023-03-23T15:45:02+01:00",
    "input_file": {}
  }
]
